\section{Categorical Empty Dictionary Architecture}
\label{sec:dictionary}

\subsection{Dictionary Structure}

We define a dynamic dictionary $\mathcal{D}$ as a tuple:
\begin{equation}
\mathcal{D} = (\mathcal{E}, \mathcal{C}, \mathcal{T})
\end{equation}
where $\mathcal{E}$ is the set of dictionary entries, $\mathcal{C}$ is the set of equivalence classes, and $\mathcal{T}$ is the KD-tree index for fast lookup.

\begin{definition}[Dictionary Entry]
A dictionary entry $e \in \mathcal{E}$ is:
\begin{equation}
e = (s, n, m, \mathbf{S}, R, \mu, c, d)
\end{equation}
where:
\begin{itemize}
\item $s$ is the symbol (single letter code)
\item $n$ is the full name
\item $m$ is the monoisotopic mass
\item $\mathbf{S} \in \mathcal{S}^3$ is the S-Entropy coordinate vector
\item $R$ is the set of fragmentation rules
\item $\mu$ is the metadata dictionary
\item $c \in [0, 1]$ is the confidence
\item $d \in \{\text{standard}, \text{learned}\}$ is the discovery method
\end{itemize}
\end{definition}

\subsection{Equivalence Classes}

\begin{definition}[S-Entropy Equivalence Class]
An equivalence class $C \in \mathcal{C}$ groups entries with similar S-Entropy coordinates:
\begin{equation}
C = (id, \mathbf{S}_c, r, M)
\end{equation}
where $\mathbf{S}_c$ is the class centroid, $r$ is the class radius, and $M$ is the member count.
\end{definition}

Entry $e$ belongs to class $C$ if:
\begin{equation}
\|\mathbf{S}(e) - \mathbf{S}_c\| \leq r
\end{equation}

\subsection{KD-Tree Index}

The dictionary maintains a KD-tree $\mathcal{T}$ over the S-Entropy coordinates of all entries for $O(\log |\mathcal{E}|)$ nearest-neighbor lookup.

\begin{algorithm}[H]
\caption{Dictionary Lookup}
\label{alg:dict_lookup}
\begin{algorithmic}[1]
\Procedure{Lookup}{$\mathcal{D}$, $\mathbf{S}_q$, $k$, $r_{\max}$}
    \State RebuildKDTree($\mathcal{D}$) if dirty
    \State (Distances, Indices) $\gets \mathcal{T}$.Query($\mathbf{S}_q$, $k$)
    \State Results $\gets$ EmptyList()

    \For{$i \in \{1, \ldots, k\}$}
        \If{Distances[$i$] $\leq r_{\max}$ \textbf{or} $r_{\max} =$ null}
            \State $e \gets \mathcal{E}$[Indices[$i$]]
            \State Results.Append(($e$, Distances[$i$]))
        \EndIf
    \EndFor

    \State \Return Results
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Dynamic Learning}

The dictionary supports dynamic learning of novel molecular entities.

\begin{definition}[Novel Entry Learning]
For an observed S-Entropy coordinate $\mathbf{S}_{\text{obs}}$ and mass $m_{\text{obs}}$ with no matching dictionary entry:
\begin{equation}
e_{\text{new}} = (s_{\text{gen}}, n_{\text{gen}}, m_{\text{obs}}, \mathbf{S}_{\text{obs}}, \varnothing, \varnothing, c_{\text{obs}}, \text{learned})
\end{equation}
where $s_{\text{gen}}$ and $n_{\text{gen}}$ are generated identifiers.
\end{definition}

\begin{algorithm}[H]
\caption{Learn Novel Entry}
\label{alg:learn_novel}
\begin{algorithmic}[1]
\Procedure{LearnNovel}{$\mathcal{D}$, $\mathbf{S}$, $m$, $c$}
    \State $n_{\text{novel}} \gets |\{e \in \mathcal{E} : e.d = \text{learned}\}| + 1$
    \State $s \gets$ ``X'' + ToString($n_{\text{novel}}$)
    \State $n \gets$ ``Novel\_'' + ToString($n_{\text{novel}}$)

    \State $e \gets$ CreateEntry($s$, $n$, $m$, $\mathbf{S}$, $c$, ``learned'')
    \State $\mathcal{D}$.AddEntry($e$)

    \State \Return $e$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Zero-Shot Identification}

\begin{definition}[Zero-Shot Identification]
Given query coordinates $\mathbf{S}_q$ and mass $m_q$, zero-shot identification returns:
\begin{equation}
e^* = \arg\min_{e \in \mathcal{E}} \|\mathbf{S}(e) - \mathbf{S}_q\| \quad \text{subject to} \quad |m(e) - m_q| \leq \epsilon_m
\end{equation}
\end{definition}

\begin{algorithm}[H]
\caption{Zero-Shot Identification}
\label{alg:zero_shot}
\begin{algorithmic}[1]
\Procedure{ZeroShotIdentify}{$\mathcal{D}$, $\mathbf{S}_q$, $m_q$, $\epsilon_S$, $\epsilon_m$}
    \State Candidates $\gets$ Lookup($\mathcal{D}$, $\mathbf{S}_q$, $k=5$, $\epsilon_S$)
    \State Filtered $\gets$ EmptyList()

    \For{($e$, $d$) $\in$ Candidates}
        \If{$|m(e) - m_q| \leq \epsilon_m$}
            \State Filtered.Append(($e$, $d$))
        \EndIf
    \EndFor

    \If{Filtered is empty}
        \State \Return (null, 0.0)
    \EndIf

    \State ($e^*$, $d^*$) $\gets$ Filtered[0]
    \State $c \gets \exp(-d^* / \sigma)$
    \State \Return ($e^*$, $c$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Empty Dictionary Principle}

The ``empty dictionary'' terminology reflects the principle that the dictionary begins with minimal content (standard amino acids) and grows dynamically through learning, converging toward complete molecular vocabulary through equilibrium-seeking behavior.

\begin{theorem}[Dictionary Convergence]
Under repeated observation of molecular entities, the dictionary entry set $\mathcal{E}$ converges to a fixed point:
\begin{equation}
\lim_{t \to \infty} \mathcal{E}^{(t)} = \mathcal{E}^*
\end{equation}
where $\mathcal{E}^*$ contains all entities observed with sufficient confidence.
\end{theorem}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/dictionary_atlas.png}
\caption{\textbf{Categorical dictionary architecture and learned amino acid organization.}
(\textbf{Top-left}) Three-dimensional scatter plot of dictionary entries in S-Entropy
space (S$_k$, S$_t$, S$_e$). Each sphere represents one learned amino acid, colored
by molecular mass (viridis colormap: yellow = heavy, purple = light) and labeled
with single-letter code. Spatial distribution shows natural clustering: hydrophobic
residues (I, L, V, M, F, W) cluster at high S$_k$ (right), charged residues (K, R,
D, E) at low S$_k$ (left), and special residues (G, P, C) at low S$_t$ (bottom).
This organization enables efficient KD-tree nearest-neighbor lookup (Section 4.3).
(\textbf{Top-right}) Equivalence class network showing amino acids grouped by
S-Entropy similarity (Euclidean distance < 0.3, Equation 17). Nodes represent
amino acids (colored by equivalence class), with edges connecting similar entries.
Network reveals natural clustering: hydrophobic cluster (teal, right), charged
cluster (pink, top-right), polar cluster (gray, center), aromatic cluster (green,
bottom-right), and special cluster (yellow, top-left). These equivalence classes
enable categorical completion (Section 5.3) by identifying interchangeable amino
acids.
(\textbf{Bottom-left}) Discovery method distribution (pie chart). All dictionary
entries (100\%) were initialized from standard amino acid definitions, shown in
blue. This demonstrates the "empty dictionary" principle (Section 4.6), where the
system starts with minimal knowledge and can learn novel entities dynamically
through equilibrium-seeking dynamics (Equation 18).
(\textbf{Bottom-right}) Confidence versus molecular mass scatter plot. Each point
represents one dictionary entry, colored by S-Entropy (viridis colormap) and labeled
with amino acid symbol. All entries show confidence = 1.0 (top of plot), indicating
high-quality learned representations. Mass range spans from Glycine (G, 57 Da) to
Tryptophan (W, 186 Da), covering the full standard amino acid spectrum. The uniform
high confidence validates dictionary quality for zero-shot identification (Section 4.5).
This comprehensive atlas demonstrates that the categorical dictionary (Definition 4)
organizes amino acids in a structured S-Entropy space, enabling efficient lookup,
equivalence class formation, and dynamic learning. The spatial organization validates
using KD-tree indexing (Section 4.3) for O(log N) identification complexity, a key
computational advantage over traditional database methods.}
\label{fig:dictionary_atlas}
\end{figure}

\subsection{Persistence}

The dictionary supports serialization for persistence:
\begin{equation}
\text{Save}: \mathcal{D} \rightarrow \text{JSON}
\end{equation}
\begin{equation}
\text{Load}: \text{JSON} \rightarrow \mathcal{D}
\end{equation}

This enables incremental learning across analysis sessions.
